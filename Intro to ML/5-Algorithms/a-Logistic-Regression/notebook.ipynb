{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![@mikegchambers](../../images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this notebook, we explore Logistic Regression using scikit-learn.\n",
    "\n",
    "![Binary](binary.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import style\n",
    "style.use('ggplot') or plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that we can easily play with the data, let's define it out in the open like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "y = [0,0,0,0,0,0,0,0,1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "X = np.reshape(X, (-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, of course, lets have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.axes()\n",
    "\n",
    "axes.scatter(X, y, color='gray', s=50)\n",
    "\n",
    "plt.ylim(-0.1, 1.1)\n",
    "\n",
    "plt.yticks([0, 0.5, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "`C` : float, default=1.0_\n",
    "- _Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization._\n",
    "\n",
    "`solver` : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "- _Algorithm to use in the optimization problem._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=1000000, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the model.  Let's classify a new point and see what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 2.5\n",
    "pred = model.predict([[test]])\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the prediction probabilites.  We will see the probaility of classification for with 0, or 1.  The model has a build in threshold of 0.5, that's why in the previous code we recived a simple classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 2.5\n",
    "pred = model.predict_proba([[test]])\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to be able to visualise the sigmoid function on a graph one method is to get the prediction probabiliities for sequence of values and plot the results.  It's a 'brute force' method, but it works.\n",
    "\n",
    "So let's get some test values, as a list of lists that we can pass to the model for a batch prediction.   We use the max of the original test data as a limit as we only need the sigmoid plotted in relation to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(0, np.amax(X), 100)\n",
    "X_test = np.reshape(X_test, (100, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get all the predictions for these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.axes()\n",
    "\n",
    "axes.scatter(X, y, color='gray', s=50)\n",
    "axes.plot(X_test, probs[:,1], linewidth=2)\n",
    "axes.plot(X_test, (probs[:,0] + probs[:,1]) / 2, linewidth=1, c=\"green\")\n",
    "\n",
    "plt.ylim(-0.1, 1.1)\n",
    "\n",
    "plt.yticks([0, 0.5, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
