{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![@mikegchambers](../../images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression / Stochastic Gradient Descent\n",
    "\n",
    "In this notebook, we explore Linear Regression using scikit-learn.\n",
    "\n",
    "![Linear](linear.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import style\n",
    "style.use('ggplot') or plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_regression(100, 1, noise=5, bias=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.axes()\n",
    "\n",
    "axes.scatter(x=X, y=y, c='green')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The brute force 'by-hand' way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our own code to find a model using the 'calculate all the values' 'brute force method'.  Its 'slow' but it will get there-ish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a list of all the gradients we wan to try out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_test = np.arange(0, 150, 10)\n",
    "print(grad_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot them on a graph so we can see what they all look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.axes()\n",
    "\n",
    "X_test = np.linspace(X.min(),X.max())\n",
    "\n",
    "axes.scatter(x=X, y=y, c='green', zorder=1000)\n",
    "\n",
    "for grad in grad_test:\n",
    "    grad_y = grad*X_test\n",
    "    axes.plot(X_test, grad_y)\n",
    "    \n",
    "axes.set_xlim(X.min(),X.max())\n",
    "axes.set_ylim(y.min(),y.max())\n",
    "\n",
    "plt.title('All the grads to test.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our own, super simple loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ssr_loss(grad, X, y):\n",
    "    \n",
    "    # Create a var to keep track of Sum of Square Residuals\n",
    "    ssr = 0\n",
    "    \n",
    "    # Combine X and y data together, makes it easier to loop through\n",
    "    data = np.stack((X[:,0],y), axis=1)\n",
    "    \n",
    "    # Loop through all the data and calculate the distance between the \n",
    "    # data point and the line at that point on the X axis.\n",
    "    for d in data:\n",
    "        line_point = d[0]*grad\n",
    "        diff = line_point - d[1]\n",
    "        \n",
    "        # And add this to the Sum of Square Residuals\n",
    "        ssr = ssr + (diff * diff)\n",
    "        \n",
    "    # All done.  Send the result back.\n",
    "    return ssr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of our test gradients, lets calculate a loss.\n",
    "\n",
    "First let's make some places to store what we find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_line = []\n",
    "min_ssr = 1e100\n",
    "min_grad = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets perform the brute force bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grad in grad_test:\n",
    "\n",
    "    ssr = simple_ssr_loss(grad, X, y)\n",
    "\n",
    "    square_line.append(ssr)\n",
    "    if ssr < min_ssr:\n",
    "        min_ssr = ssr\n",
    "        min_grad = grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to plot the results.  We should get a nice curve, and as we have them, we can also plot some lines to show the minima, thats the bit we were looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.axes()\n",
    "\n",
    "plt.plot(grad_test,square_line)\n",
    "\n",
    "plt.axvline(min_grad, c='green')\n",
    "plt.axhline(min_ssr, c='green')\n",
    "\n",
    "plt.ylabel('SSR')\n",
    "plt.xlabel('Gradient')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to be clear, let's print the value of the gradient we found to fit best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for this brute force atempt, lets show the line we found, with the original data.  How does it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.axes()\n",
    "\n",
    "# Plot the data points:\n",
    "axes.scatter(x=X, y=y, c=\"green\")\n",
    "\n",
    "# Get the slope and the x intercept of the model line:\n",
    "grad = min_grad\n",
    "\n",
    "# Plot the line (remember y=mx+c?):\n",
    "x_line = np.linspace(X.min(),X.max())\n",
    "y_line = grad*x_line\n",
    "axes.plot(x_line, y_line)\n",
    "\n",
    "plt.title('Model Line by Brute Force')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The scikit-learn way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now lets do it a way more effient way, using scikit-learn's LinearRegression / Stochastic Gradient Descent.  Much fewer lines for us to write, faster to run, and usually more accurate too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "# model = SGDRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!  Now let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.axes()\n",
    "\n",
    "# Plot the data points:\n",
    "axes.scatter(x=X, y=y)\n",
    "\n",
    "# axes.set_xlim(X.min(),X.max())\n",
    "# axes.set_ylim(y.min(),y.max())\n",
    "\n",
    "# Get the slope and the x intercept of the model line:\n",
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "\n",
    "# Plot the line (remember y=mx+c?):\n",
    "x_line = np.linspace(X.min(),X.max())\n",
    "y_line = slope*x_line+intercept\n",
    "axes.plot(x_line, y_line, 'red')\n",
    "\n",
    "plt.title('Model Line by scikit-learn')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to be clear, let's print the value of the gradient scikit-learn found to fit best.  How different is it from our manual attempt?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
